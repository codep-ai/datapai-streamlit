# DataPAI â€” LLM & Vector-DB Architecture Guide

When to use which model, which database, and how cost is controlled.

---

## 1. The Three Execution Paths

Every AI feature in DataPAI follows one of three patterns:

```
User question
     â”‚
     â”œâ”€ Path A: Quick Interpret  â”€â”€â–º  Paid LLM (no vector DB)
     â”‚
     â”œâ”€ Path B: RAG Query        â”€â”€â–º  Vector DB read  â†’  Local or Paid LLM
     â”‚
     â””â”€ Path C: Ingest           â”€â”€â–º  Vector DB write  (no LLM)
```

---

## 2. Path A â€” Quick Interpret (low-latency, paid LLM)

Use this when you need a **fresh answer from a document right now** and haven't ingested it yet.

```
ASX API â†’ PDF download (in-memory) â†’ pdfplumber â†’ text injected into prompt
                                                           â”‚
                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                               Step 1: Gemini flash-lite
                                    (primary extraction)
                                          â”‚ draft
                               Step 2: GPT-5.1
                                    (reviewer â€” fires only when needed)
                                          â”‚
                                    Final answer â†’ User
```

### Latency breakdown

| Step | Duration |
|------|----------|
| ASX API list call | ~0.5â€“1 s |
| PDF download | ~1â€“4 s (100 KBâ€“5 MB) |
| pdfplumber extraction | ~0.1â€“0.5 s |
| Gemini flash-lite draft | ~1â€“2 s |
| GPT-5.1 reviewer | ~3â€“8 s |
| **Total** | **~6â€“15 s** |

### Why Gemini first, GPT second?

- The document text is injected **verbatim** â€” the primary model needs to **read**, not **recall**
- Gemini flash-lite handles extraction at ~1â€“2 s and ~20Ã— lower cost than GPT-5.1
- GPT-5.1 as **reviewer** is a stronger quality gate: it catches wrong figures or missed sentiment
- The reviewer fires **conditionally** â€” replies `APPROVED` (return draft) or rewrites; most calls cost only one Gemini round-trip

### When to use

- Ad-hoc analysis of a freshly published announcement
- Single-document Q&A where RAG overhead isn't justified
- When the document hasn't been ingested yet

---

## 3. Path B â€” RAG Query (vector DB + local or paid LLM)

Use this for **repeated questions over a large corpus** of previously ingested documents.

```
User question
     â”‚
     â–¼
HuggingFace all-MiniLM-L6-v2  (local embed, ~100ms)
     â”‚
     â–¼
LanceDB ANN similarity search  (vector DB read, ~200msâ€“1s)
     â”‚   returns top-k chunks
     â–¼
LLM generation (RouterChatClient)
     â”‚
     â”œâ”€ LLM_MODE=local   â†’  Ollama on EC2 #3 GPU  (free, ~3â€“15 s)
     â”œâ”€ LLM_MODE=paid    â†’  OpenAI GPT-5.1        (paid, ~3â€“8 s)
     â””â”€ LLM_MODE=hybrid  â†’  Ollama â†’ GPT fallback
```

### When to use

- Historical analysis across many documents (e.g. "What did BHP say about iron ore in FY24?")
- Follow-up chat after ingesting a batch of announcements
- Privacy-sensitive environments â†’ use `LLM_MODE=local` (Ollama, no data leaves your VPC)

### Cost comparison vs Path A

| | Path A (Quick Interpret) | Path B (RAG) |
|--|--------------------------|--------------|
| Per-query LLM cost | Gemini + GPT per call | Embed once; Ollama free after |
| Best for | Single fresh document | Many queries over ingested corpus |
| Data leaves VPC? | Yes (Gemini + OpenAI) | Only in `LLM_MODE=paid` |

---

## 4. Path C â€” Ingest (vector DB write, no LLM)

Embedding only â€” no LLM involved, essentially free.

```
PDF text
     â”‚
     â–¼
HuggingFace all-MiniLM-L6-v2  (local, CPU)  ~0.1â€“0.5 s
     â”‚  384-dim vector
     â–¼
LanceDB table  (asx_announcements / pdfs / documents / images)
     â”‚
  Deduplication check (filename column) â†’ skip if already ingested
```

**Cost:** ~$0 (HuggingFace model runs locally via sentence-transformers).
**Time per document:** ~2â€“7 s.

---

## 5. LLM Routing â€” `RouterChatClient`

The `RouterChatClient` in `agents/llm_client.py` applies to all non-ASX agents.
ASX interpretation uses a **separate, fixed chain** (Gemini â†’ GPT) regardless of `LLM_MODE`.

### `LLM_MODE` env var

| Value | Primary | Secondary | Use when |
|-------|---------|-----------|----------|
| `paid` | OpenAI GPT-5.1 | Google Gemini (reviewer, optional) | Best accuracy, demo |
| `local` | Ollama | â€” | Air-gapped / privacy-first |
| `hybrid` | Ollama â†’ GPT fallback | â€” | Maximise local, allow cloud fallback |

### `LLM_DUAL_REVIEW=1`

When enabled, every RouterChatClient call sends the primary answer to a secondary model
(configured via `LLM_SECONDARY_PROVIDER`) for a JSON approve/rewrite review.
Adds ~3â€“8 s latency. Recommended for SQL generation, not for streaming chat.

---

## 6. Vector DB â€” LanceDB Collections

| Collection | Contents | Written by | Read by |
|------------|----------|------------|---------|
| `pdfs` | Manually uploaded PDFs (Streamlit tab 4) | `knowledge_ingest_agent.py` | `knowledge_query_agent.py` |
| `documents` | General documents (CSV, XLSX, TXT) | `knowledge_ingest_agent.py` | `knowledge_query_agent.py` |
| `images` | OCR-extracted image text | `knowledge_ingest_agent.py` | `knowledge_query_agent.py` |
| `asx_announcements` | ASX PDF announcements | `asx_announcement_agent.py` | `knowledge_query_agent.py` + `/v1/rag/retrieve` |

All collections are stored at `LANCEDB_URI` (default: `s3://codepais3/lancedb_data/`).
The embedding model is **all-MiniLM-L6-v2** (384 dims) for all collections.

---

## 7. Cost Guard â€” Daily Budget Enforcement

### Overview

`agents/cost_guard.py` tracks cumulative USD spend against a daily ceiling.
When the ceiling is reached, `BudgetExceededError` is raised **before** the next API call.

```
OpenAIChatClient.chat()                GoogleChatClient.chat()
  _guard.check(model)  â† raises here     _guard.check(model)
  â†’ OpenAI API call                       â†’ Gemini API call
  _guard.record(model, in, out)          _guard.record(model, in, out)
         â”‚                                        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ /tmp/datapai_cost_YYYY-MM-DD.json â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### State file

`/tmp/datapai_cost_YYYY-MM-DD.json`

Date-stamped â€” automatically stale at midnight, no cleanup needed.

```json
{"date": "2026-02-28", "spend_usd": 1.2345, "calls": 14}
```

### Pricing table (approximate â€” conservative)

| Model | Input $/1M | Output $/1M |
|-------|-----------|------------|
| gemini-2.5-flash-lite | $0.10 | $0.40 |
| gemini-2.5-flash | $0.15 | $0.60 |
| gpt-5.1 / gpt-4.1 | $2.00 | $8.00 |
| gpt-4o | $2.50 | $10.00 |
| gpt-4o-mini | $0.15 | $0.60 |
| claude-3-5-sonnet (Bedrock) | $3.00 | $15.00 |

### Environment variables

| Variable | Default | Description |
|----------|---------|-------------|
| `DAILY_LLM_BUDGET_USD` | `5.00` | Daily ceiling in USD |
| `COST_GUARD_ENABLED` | `true` | Set `false` to disable for production |

### Monitoring

```bash
# REST API
curl http://localhost:8100/v1/cost/status

# Example response
{
  "enabled": true,
  "budget_usd": 5.0,
  "spent_today": 1.2345,
  "remaining_usd": 3.7655,
  "calls_today": 14,
  "date": "2026-02-28",
  "pct_used": 24.7
}
```

The Streamlit sidebar also shows a live progress bar with spend / remaining / call count.

### What happens when budget is exhausted

1. `_guard.check()` raises `BudgetExceededError` (subclass of `RuntimeError`)
2. The calling agent catches it and returns a user-facing message:
   `"ğŸ’¸ Daily LLM budget of $5.00 reached (spent today: $5.0012)..."`
3. Ollama (`LLM_MODE=local`) is **unaffected** â€” Ollama calls are never metered

---

## 8. EC2 Architecture

```
EC2 #1 â€” Streamlit frontend (app_ai_agent.py)
  â””â”€ calls EC2 #2 RAG API  (port 8100)
  â””â”€ calls EC2 #2 SQL API  (port 8101)

EC2 #2 â€” FastAPI services (CPU)
  â”œâ”€ agents/rag_api.py       port 8100
  â”‚     /v1/rag/retrieve      LanceDB ANN search â†’ context chunks
  â”‚     /v1/rag/ingest        embed + store to LanceDB
  â”‚     /v1/asx/interpret     PDF download + Geminiâ†’GPT chain
  â”‚     /v1/asx/ingest        embed ASX PDFs to LanceDB
  â”‚     /v1/cost/status       today's spend vs budget
  â”‚
  â””â”€ agents/text2sql_api.py  port 8101
        /v1/sql/query          natural language â†’ SQL â†’ execute â†’ results

EC2 #3 â€” Ollama (GPU)
  â””â”€ llama3.2 / deepseek-coder  (LLM_MODE=local or hybrid fallback)

External paid APIs
  â”œâ”€ Google Gemini (generativelanguage.googleapis.com)  â€” GUARDED
  â””â”€ OpenAI GPT-5.1 (api.openai.com)                   â€” GUARDED
```

---

## 9. Decision Flowchart â€” Which path to use?

```
Do you need a fresh answer from a document that hasn't been ingested?
  YES â†’ Path A (Quick Interpret)   ~6â€“15 s, two paid API calls
  NO  â†“

Has the document already been ingested to LanceDB?
  YES â†’ Path B (RAG Query)
  NO  â†’ Ingest first (Path C), then Path B

For Path B â€” is data privacy a concern?
  YES â†’ LLM_MODE=local (Ollama, no cloud egress)
  NO  â†’ LLM_MODE=paid (OpenAI, faster, higher quality)

Are you asking the same question repeatedly over many documents?
  YES â†’ Ingest all (Path C) once, then RAG (Path B) is ~$0 per query
  NO  â†’ Quick Interpret (Path A) per document as needed
```

---

## 10. Quick Reference â€” Environment Variables

```bash
# LLM routing
LLM_MODE=paid                  # paid | local | hybrid
LLM_PRIMARY_PROVIDER=openai    # openai | bedrock
LLM_SECONDARY_PROVIDER=google  # openai | bedrock | google
LLM_DUAL_REVIEW=1              # 1=enable second-pass review

# Models
OPENAI_MODEL=gpt-5.1
GOOGLE_MODEL=gemini-2.5-flash-lite
BEDROCK_MODEL_ID=anthropic.claude-3-5-sonnet-20240620-v1:0
OLLAMA_MODEL=llama3.2

# Cost guard
DAILY_LLM_BUDGET_USD=5.00      # daily ceiling, resets at midnight
COST_GUARD_ENABLED=true        # false to disable

# Storage
LANCEDB_URI=s3://codepais3/lancedb_data/

# Services
OLLAMA_HOST=http://localhost:11434
DATAPAI_RAG_API_URL=http://localhost:8100
DATAPAI_SQL_API_URL=http://localhost:8101
```
